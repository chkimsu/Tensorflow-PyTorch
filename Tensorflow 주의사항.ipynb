{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### tf.metrics 누적이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 is the mean of [1]\n",
      "1.5 is the mean of [1, 2]\n",
      "2.0 is the mean of [1, 2, 3]\n",
      "2.5 is the mean of [1, 2, 3, 4]\n",
      "3.0 is the mean of [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mean_object = tf.metrics.Mean()\n",
    "\n",
    "\n",
    "### 자동으로 누적 가중평균을 하는 방식인듯하다. \n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "for ix, val in enumerate(values):\n",
    "    mean_object.update_state(val)\n",
    "    print(mean_object.result().numpy(), 'is the mean of', values[:ix+1])\n",
    "\n",
    "mean_object.reset_states()\n",
    "mean_object.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 is the mean of [1]\n",
      "1.5 is the mean of [1, 2]\n",
      "2.0 is the mean of [1, 2, 3]\n",
      "2.5 is the mean of [1, 2, 3, 4]\n",
      "3.0 is the mean of [1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mean_object = tf.metrics.Mean()\n",
    "\n",
    "\n",
    "### 자동으로 누적 가중평균을 하는 방식인듯하다. \n",
    "### __call__ 메서드에서도 동일하다. update인듯하다. \n",
    "\n",
    "values = [1, 2, 3, 4, 5]\n",
    "\n",
    "for ix, val in enumerate(values):\n",
    "    mean_object(val)\n",
    "    print(mean_object.result().numpy(), 'is the mean of', values[:ix+1])\n",
    "\n",
    "mean_object.reset_states()\n",
    "mean_object.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실전 예시다. 아래에서. loss_metric(loss)를 통해 배치단위로 계속해서 loss가 저장되므로 이렇게 해도 되는것이다. \n",
    "\n",
    "\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch {}'.format(epoch))\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses) # Add KLD regularization loss\n",
    "        \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        \n",
    "        loss_metric(loss)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('step {}: mean loss = {}'.format(step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### tf.Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "()\n",
      "tf.Tensor(\n",
      "[[8. 8.]\n",
      " [8. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### tf.GradientTape 의 경우 한번 t.gradient 쓰면 끝나버리기때문에 persistent=True로 해줘야한다. 하지만 메모리 많이 차지하므로 나중에 del t해주고\n",
    "\n",
    "import tensorflow as tf\n",
    "# Define a 2x2 array of 1's\n",
    "x = tf.ones((2,2))\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "# Record the actions performed on tensor x with `watch`\n",
    "    t.watch(x)\n",
    "# Define y as the sum of the elements in x\n",
    "    y = tf.reduce_sum(x)\n",
    "# Let z be the square of y\n",
    "    z = tf.square(y)\n",
    "# Get the derivative of z wrt the original input tensor x\n",
    "    dz_dx = t.gradient(z, x)\n",
    "\n",
    "    print(t.watched_variables())\n",
    "# Print our result\n",
    "print(dz_dx)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7e89925b1aa50b605f20d7e318acc682aa3f2d8d9d7bfa0a81657dbff3df5c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
