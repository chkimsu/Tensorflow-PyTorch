{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in Mind - A LightningModule is a PyTorch nn.Module - it just has a few more helpful features.\n",
    "## training_step과 configure_optimizers 필수적으로 구현해야 합니다.\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "\n",
    "\n",
    "class MNISTModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "## training step에서는 꼭 log뿐만 아니라 return loss를 해줘야한다\n",
    "# self.log하는거는 매번 다 logging되고 이를 callback에서 활용할 수 있다. \n",
    "## validation step에서는 할 필요 없지만. \n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        loss = F.cross_entropy(self(x), y)\n",
    "        # return loss\n",
    "        self.log('chkimsu_loss',loss * 2, prog_bar = True, on_step= True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chkim\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000023D1C093848>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000023D1C093848>)`.\n",
      "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 7.9 K \n",
      "--------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.031     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 938/938 [00:09<00:00, 95.80it/s, loss=0.683, v_num=17, chkimsu_loss=0.795] \n"
     ]
    }
   ],
   "source": [
    "# Init our model\n",
    "mnist_model = MNISTModel()\n",
    "import pytorch_lightning\n",
    "\n",
    "checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint('20220427',\n",
    "                                                  save_top_k=2, monitor='chkimsu_loss', mode='min', save_last = True)\n",
    "\n",
    "\n",
    "# Init DataLoader from MNIST Dataset\n",
    "train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=10,\n",
    "    progress_bar_refresh_rate=20,\n",
    "    checkpoint_callback = checkpoint_callback\n",
    ")\n",
    "    \n",
    "# Train the model ⚡\n",
    "trainer.fit(mnist_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    #   forward는 모델의 추론 결과를 제공하고 싶을 때 사용합니다. \n",
    "    #   nn.Module처럼 꼭 정의해야 하는 메서드는 아니지만 self(<입력>)과 같이 사용할 수 있게 만들어주므로 구현해주면 다른 메서드를 구현할 때 편리합니다.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    # training_step은 학습 루프의 body 부분을 나타냅니다. \n",
    "    # 이 메소드에서는 argument로 training 데이터로더가 제공하는 batch와 해당 batch의 인덱스가 주어지고 학습 로스를 계산하여 리턴합니다.\n",
    "    #  pytorch lightning은 편리하게도 batch의 텐서를 cpu 혹은 gpu 텐서로 변경하는 코드를 따로 추가하지 않아도 trainer의 설정에 따라 자동으로 적절한 타입으로 변경해줍니다.\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "#   validation_step은 학습 중간에 모델의 성능을 체크하는 용도로 사용합니다.\n",
    "#    training_step과 마찬가지로 validation 데이터로더에서 제공하는 배치를 가지고 확인하고자 하는 통계량을 기록할 수 있습니다.\n",
    "#     하나의 값을 저장할 때는 self.log(<변수 이름=\"\">, <값>)과 같이 저장할 수 있고\n",
    "#     여러 개의 변수를 저장하고 싶으면 아래 예시와 같이 self.log_dict로 변수 이름, 값 쌍을 가지고 있는 딕셔너리를 저장할 수 있습니다.\n",
    "#      각 스탭마다 변수에 저장된 값의 평균이 해당 변수의 최종 값이 됩니다. 특별히 설정을 바꾸지 않으면 변수 중에 'val_loss'가 best 모델을 구하는 기준으로 사용됩니다.\n",
    "## !!! 중요한 건 VALIDATION STEP의 마지막에 저장되는 값은 STEP마다 평균내진다는 것이ㅏㄷ. \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        acc = FM.accuracy(logits, y)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        acc = FM.accuracy(logits, y)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        metrics = {'test_acc': acc, 'test_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters())\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(args.seed)\n",
    "\n",
    "# dataloaders\n",
    "dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset, val_dataset = random_split(dataset, [55000, 5000])\n",
    "test_dataset = MNIST('', train=False, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습하기 위해서는 학습 로직을 정하는 Trainer를 생성해야 합니다. Pytorch lightning의 Trainer는 굉장히 많은 기능을 제공합니다. \n",
    "# 아래 예제에서는 간단히 학습 epoch 수와 gpu 수만 조정할 수 있도록 만들었습니다.\n",
    "\n",
    "# gpus가 0일 때는 cpu를 사용하고 gpus가 1 이상이면 gpu를 사용하여 모델을 학습합니다.\n",
    "# gpus가 2 이상이면 자동으로 다중 gpu를 활용해 분산 학습을 진행하게 되는데 기본 설정은 process를 spawn하는 distributed data parallel 방식(ddp_spawn)으로 되어있습니다.\n",
    "\n",
    "# model\n",
    "model = Classifier()\n",
    "\n",
    "# training\n",
    "trainer = pl.Trainer(max_epochs=args.n_epochs, gpus=args.n_gpus)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Trainer의 test 함수에 test 데이터로더를 넘겨주어 모델을 테스트할 수 있습니다. \n",
    "# 아래와 같이 따로 어떤 모델을 테스트할지 지정하지 않으면 자동으로 Trainer가 validation을 통해 구한 best 모델을 가지고 테스트를 진행하게 됩니다.\n",
    "trainer.test(test_dataloaders=test_loader)\n",
    "\n",
    "\n",
    "## PL은 기본적으로 CHECKPOINT를 매 버전마다 저장해주지만 이 저장주기를 바꾸고 싶을때 CALLBACK을 이용하는 것!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Lightning Model 정의를 할 클래스에는 반드시 LightningModule을 상속받는다. (Like Torch's nn.Module)\n",
    "# pretrained model을 생성하고 transfer learning을 위해 마지막 Linear layer의 출력을 1(for binary)로 바꿔준다.\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net = EfficientNet.from_pretrained(arch, advprop=True)\n",
    "        self.net._fc = nn.Linear(in_features=self.net._fc.in_features, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):  #model의 입력에 대한 output을 내는 forward\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):  # 최적화를 위한 optimizer와 learning rate scheduler 초기화 및 반환\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            max_lr=lr,\n",
    "            epochs=max_epochs,\n",
    "            optimizer=optimizer,\n",
    "            steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "            pct_start=0.1,\n",
    "            div_factor=10,\n",
    "            final_div_factor=100,\n",
    "            base_momentum=0.90,\n",
    "            max_momentum=0.95,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "    def step(self, batch):  # forward and calculate loss\n",
    "        # return batch loss\n",
    "        x, y  = batch\n",
    "        y_hat = self(x).flatten()\n",
    "        y_smo = y.float() * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "        loss  = F.binary_cross_entropy_with_logits(y_hat, y_smo.type_as(y_hat),\n",
    "                                                   pos_weight=torch.tensor(pos_weight))\n",
    "        return loss, y, y_hat.sigmoid()\n",
    "\n",
    "\n",
    "    # 1 iteration에 대한 training\n",
    "    # batch만큼의 output을 얻고 loss와 accuracy를 return\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # hardware agnostic training\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        acc = (y_hat.round() == y).float().mean().item()\n",
    "        tensorboard_logs = {'train_loss': loss, 'acc': acc}\n",
    "        return {'loss': loss, 'acc': acc, 'log': tensorboard_logs}\n",
    "\n",
    "\n",
    "\n",
    "    # validation_step은 1 iteration에 대한 함수라고 하면  validation_epoch_end는 1 epoch에 대한 함수이다.\n",
    "\n",
    "    # validation_step 함수의 역할은 training_step과 같은 역할을 하며 validation_epoch_end 함수는 logging과 학습과정에 대한 print를 위해 사용한다\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        return {'val_loss': loss,\n",
    "                'y': y.detach(), 'y_hat': y_hat.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):  # 한 에폭이 끝났을 때 실행\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        y = torch.cat([x['y'] for x in outputs])\n",
    "        y_hat = torch.cat([x['y_hat'] for x in outputs])\n",
    "        auc = AUROC()(y_hat, y) if y.float().mean() > 0 else 0.5 # skip sanity check\n",
    "        acc = (y_hat.round() == y).float().mean().item()\n",
    "        print(f\"Epoch {self.current_epoch} acc:{acc} auc:{auc}\")\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_auc': auc, 'val_acc': acc}\n",
    "        return {'avg_val_loss': avg_loss,\n",
    "                'val_auc': auc, 'val_acc': acc,\n",
    "                'log': tensorboard_logs}\n",
    "\n",
    "    # 테스트이므로 정답이 없다. \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, _ = batch\n",
    "        y_hat = self(x).flatten().sigmoid()\n",
    "        return {'y_hat': y_hat}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        y_hat = torch.cat([x['y_hat'] for x in outputs])\n",
    "        df_test['target'] = y_hat.tolist()\n",
    "        N = len(glob('submission*.csv'))\n",
    "        df_test.target.to_csv(f'submission{N}.csv')\n",
    "        return {'tta': N}\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=True, shuffle=True, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=False, shuffle=False, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=False, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint('{epoch:02d}_{val_auc:.4f}',\n",
    "                                                  save_top_k=1, monitor='val_auc', mode='max')\n",
    "trainer = pl.Trainer(\n",
    "    tpu_cores=tpu_cores,\n",
    "    gpus=gpus,\n",
    "    precision=16 if gpus else 32,\n",
    "    max_epochs=max_epochs,\n",
    "    num_sanity_val_steps=1 if debug else 0,  # catches any bugs in your validation without having to wait for the first validation check. \n",
    "    checkpoint_callback=checkpoint_callback\n",
    "    )\n",
    "    \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=True, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=False, shuffle=False, pin_memory=True)          \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,\n",
    "                          drop_last=False, shuffle=False, pin_memory=False)\n",
    "                          \n",
    "def train(epoch, model, optimizer, criterion, scaler, scheduler):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f'TRAIN-{epoch}', leave=True)\n",
    "    for idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        smooth_targets = targets.float() * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs).flatten()\n",
    "            loss = criterion(outputs, smooth_targets)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        output_enc = outputs.sigmoid().round()\n",
    "        correct += (output_enc == targets).sum().item()\n",
    "        total += targets.shape[0]\n",
    "\n",
    "        mean_loss = sum(train_loss) / len(train_loss)\n",
    "        mean_acc = correct / total * 100\n",
    "        loop.set_postfix(loss=mean_loss, accuracy=mean_acc)\n",
    "        \n",
    "    scheduler.step(mean_loss) \n",
    "    \n",
    "def valid(epoch, model, optimizer, criterion):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ra = 0\n",
    "\n",
    "    loop = tqdm(valid_loader, desc=f'VALID-{epoch}', leave=True)\n",
    "    for idx, (inputs, targets) in enumerate(loop):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model(inputs).flatten() \n",
    "            loss = criterion(outputs, targets.float())\n",
    "        \n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "       \n",
    "        output_enc = outputs.sigmoid().round()\n",
    "        correct += (output_enc == targets).sum().item()\n",
    "        total += targets.shape[0]\n",
    "\n",
    "        ra += roc_auc_score(output_enc.cpu(),targets.cpu())\n",
    "        \n",
    "        mean_loss = sum(val_loss) / len(val_loss)\n",
    "        mean_acc = correct / total * 100\n",
    "        loop.set_postfix(loss=mean_loss, accuracy=mean_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} acc:{mean_acc} auc:{ra / len(train_loader)}\")\n",
    "    \n",
    "def test(model, optimizer):\n",
    "    model.eval()\n",
    "    all = torch.tensor([])\n",
    "    loop = tqdm(test_loader, desc=f'TEST', leave=True)\n",
    "    for (inputs, _) in loop:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs).flatten().sigmoid()\n",
    "        all = torch.cat((all, outputs))\n",
    "        \n",
    "    df_test['target'] = all.tolist()\n",
    "    N = len(glob('submission*.csv'))\n",
    "    df_test.target.to_csv(f'submission{N}.csv')\n",
    "    return {'tta': N}\n",
    "   \n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "def main():\n",
    "    model = EfficientNet.from_pretrained(arch, advprop=True)\n",
    "    model._fc = nn.Linear(model._fc.in_features, out_features=1, bias=True)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()  # FP16\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        max_lr=lr,\n",
    "        epochs=max_epochs,\n",
    "        optimizer=optimizer,\n",
    "        steps_per_epoch=int(len(train_dataset) / batch_size),\n",
    "        pct_start=0.1,\n",
    "        div_factor=10,\n",
    "        final_div_factor=100,\n",
    "        base_momentum=0.90,\n",
    "        max_momentum=0.95,\n",
    "        )\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train(epoch, model, optimizer, criterion, scaler, scheduler)\n",
    "        valid(epoch, model, optimizer, criterion)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7e89925b1aa50b605f20d7e318acc682aa3f2d8d9d7bfa0a81657dbff3df5c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
